---
title: "DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning"
collection: publications
permalink: /publications/2025-DemoGen/
excerpt: 'Zhengrong Xue, Shuying Deng, **Zhenyang Chen**, Yixuan Wang, Zhecheng Yuan, Huazhe Xu'
date: 2025-04-01
venue: 'Robotics, Science, and System 2025'
paperurl: 'https://arxiv.org/abs/2502.16932'
citation: # 'Your Name, You. (2010). &quot;Paper Title Number 2.&quot; <i>Journal 1</i>. 1(2).'
---

Visuomotor policies have shown great promise in robotic manipulation but often require substantial amounts of human-collected data for effective performance. A key reason underlying the data demands is their limited spatial generalization capability, which necessitates extensive data collection across different object configurations. In this work, we present DemoGen, a low-cost, fully synthetic approach for automatic demonstration generation. Using only one human-collected demonstration per task, DemoGen generates spatially augmented demonstrations by adapting the demonstrated action trajectory to novel object configurations. Visual observations are synthesized by leveraging 3D point clouds as the modality and rearranging the subjects in the scene via 3D editing. Empirically, DemoGen significantly enhances policy performance across a diverse range of real-world manipulation tasks, showing its applicability even in challenging scenarios involving deformable objects, dexterous hand end-effectors, and bimanual platforms. Furthermore, DemoGen can be extended to enable additional out-of-distribution capabilities, including disturbance resistance and obstacle avoidance.

[Website](https://demo-generation.github.io/)
